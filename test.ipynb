{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from load_data import load_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(\"Saved_Model/Model.h5\")\n",
    "loaded_model.set_weights(loaded_model.get_weights())\n",
    "matrix_size = loaded_model.layers[-2].output.shape[1]\n",
    "new_model = Model(loaded_model.inputs, loaded_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8_input (InputLayer)  [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 122, 122, 64)      3200      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 122, 122, 64)      256       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_8 (Average (None, 61, 61, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 128)       401536    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_9 (Average (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_10 (Averag (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_11 (Averag (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 2, 2, 512)         2048      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 4,273,248\n",
      "Trainable params: 4,266,208\n",
      "Non-trainable params: 7,040\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Training and Testing Sets ...\n"
     ]
    }
   ],
   "source": [
    "images, labels = load_dataset(verbose=1, mode=\"Test\")\n",
    "images = np.expand_dims(images, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 128, 128, 1)\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "decodeDict = {'151404': 'The Northern Lights', '152103': 'Smile is the key', '152253': 'Milky Way', '152254': 'Mayd Hubb meets Joe Pilgrim - Tribute to Yabby You - Panda meets Jamma Dim REMIX', '152258': 'Purple Trip', '152261': 'Danse Macabre', '152262': 'Die Brücke', '152324': 'Greenhouse On Mars', '152418': 'Snosti ti Dojdov', '152425': 'Opa Tsupa', '152480': 'A Perceptible Shift', '152543': 'BITCHEZ', '152545': 'Where the Sunshine Goes', '152568': 'Everything Dances', '152569': 'The Magic of Bamboo', '152570': 'Collapse Into Innocence', '153337': 'The Vent', '153383': 'La Samba (en public)', '153452': 'Métal Province', '153946': \"Hey, that's not normal\", '153955': 'Not Coming Home', '153956': 'Border Collie', '154303': 'In It to Win It', '154305': 'Calm Flight', '154306': 'Yummy', '154307': '12:01 AM', '154308': 'MIA', '154309': 'A1 Symphony', '154413': 'Do Easy', '154414': 'Dead Can Dance (uncensored)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Northern Lights\n",
      "Smile is the key\n",
      "Milky Way\n",
      "Mayd Hubb meets Joe Pilgrim - Tribute to Yabby You - Panda meets Jamma Dim REMIX\n",
      "Purple Trip\n",
      "Danse Macabre\n",
      "Die Brücke\n",
      "Greenhouse On Mars\n",
      "Snosti ti Dojdov\n",
      "Opa Tsupa\n",
      "A Perceptible Shift\n",
      "BITCHEZ\n",
      "Where the Sunshine Goes\n",
      "Everything Dances\n",
      "The Magic of Bamboo\n",
      "Collapse Into Innocence\n",
      "The Vent\n",
      "La Samba (en public)\n",
      "Métal Province\n",
      "Hey, that's not normal\n",
      "Not Coming Home\n",
      "Border Collie\n",
      "In It to Win It\n",
      "Calm Flight\n",
      "Yummy\n",
      "12:01 AM\n",
      "MIA\n",
      "A1 Symphony\n",
      "Do Easy\n",
      "Dead Can Dance (uncensored)\n"
     ]
    }
   ],
   "source": [
    "for i in np.unique(labels):\n",
    "    print(decodeDict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Song name:\n",
      "Do Easy\n"
     ]
    }
   ],
   "source": [
    "decodeDictReverse = {x: y for y, x in decodeDict.items()}\n",
    "recommend_wrt = input(\"Enter Song name:\\n\")\n",
    "recommend_wrt = decodeDictReverse[recommend_wrt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation is:\n",
      "Song Name: Everything Dances with value = 0.926041\n",
      "Song Name: MIA with value = 0.876097\n"
     ]
    }
   ],
   "source": [
    "prediction_anchor = np.zeros((1, matrix_size))\n",
    "count = 0\n",
    "predictions_song = []\n",
    "predictions_label = []\n",
    "counts = []\n",
    "distance_array = []\n",
    "\n",
    "# Calculate the latent feature vectors for all the songs.\n",
    "for i in range(0, len(labels)):\n",
    "    if labels[i] == recommend_wrt:\n",
    "        test_image = images[i]\n",
    "        test_image = np.expand_dims(test_image, axis=0)\n",
    "        prediction = new_model.predict(test_image)\n",
    "        prediction_anchor = prediction_anchor + prediction\n",
    "        count = count + 1\n",
    "    elif labels[i] not in predictions_label:\n",
    "        predictions_label.append(labels[i])\n",
    "        test_image = images[i]\n",
    "        test_image = np.expand_dims(test_image, axis=0)\n",
    "        prediction = new_model.predict(test_image)\n",
    "        predictions_song.append(prediction)\n",
    "        counts.append(1)\n",
    "    elif labels[i] in predictions_label:\n",
    "        index = predictions_label.index(labels[i])\n",
    "        test_image = images[i]\n",
    "        test_image = np.expand_dims(test_image, axis=0)\n",
    "        prediction = new_model.predict(test_image)\n",
    "        predictions_song[index] = predictions_song[index] + prediction\n",
    "        counts[index] = counts[index] + 1\n",
    "# Count is used for averaging the latent feature vectors.\n",
    "prediction_anchor = prediction_anchor / count\n",
    "for i in range(len(predictions_song)):\n",
    "    predictions_song[i] = predictions_song[i] / counts[i]\n",
    "    # Cosine Similarity - Computes a similarity score of all songs with respect\n",
    "    # to the anchor song.\n",
    "    distance_array.append(np.sum(prediction_anchor * predictions_song[i]) / (np.sqrt(np.sum(prediction_anchor**2)) * np.sqrt(np.sum(predictions_song[i]**2))))\n",
    "\n",
    "distance_array = np.array(distance_array)\n",
    "recommendations = 0\n",
    "\n",
    "print(\"Recommendation is:\")\n",
    "\n",
    "# Number of Recommendations is set to 2.\n",
    "while recommendations < 2:\n",
    "    index = np.argmax(distance_array)\n",
    "    value = distance_array[index]\n",
    "    print(\"Song Name: \" + decodeDict[predictions_label[index]] + \" with value = %f\" % value)\n",
    "    distance_array[index] = -np.inf\n",
    "    recommendations = recommendations + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
